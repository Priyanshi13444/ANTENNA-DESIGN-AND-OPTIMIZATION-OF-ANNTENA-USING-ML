{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ac4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xlrd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "991fcd2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>Resultant Frequency</th>\n",
       "      <th>Return Loss</th>\n",
       "      <th>Fitness Function</th>\n",
       "      <th>S/N ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.89</td>\n",
       "      <td>70.20</td>\n",
       "      <td>53.83</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.270</td>\n",
       "      <td>2.58</td>\n",
       "      <td>-18.99</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>17.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.89</td>\n",
       "      <td>58.50</td>\n",
       "      <td>49.07</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.515</td>\n",
       "      <td>2.58</td>\n",
       "      <td>-10.89</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>17.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55.89</td>\n",
       "      <td>46.80</td>\n",
       "      <td>44.17</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4.545</td>\n",
       "      <td>2.32</td>\n",
       "      <td>-19.98</td>\n",
       "      <td>0.13</td>\n",
       "      <td>17.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55.89</td>\n",
       "      <td>35.10</td>\n",
       "      <td>39.26</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.790</td>\n",
       "      <td>2.48</td>\n",
       "      <td>-18.94</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>30.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.89</td>\n",
       "      <td>23.40</td>\n",
       "      <td>58.89</td>\n",
       "      <td>0.62</td>\n",
       "      <td>3.030</td>\n",
       "      <td>2.66</td>\n",
       "      <td>-22.45</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>13.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>46.57</td>\n",
       "      <td>70.20</td>\n",
       "      <td>49.07</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.790</td>\n",
       "      <td>2.34</td>\n",
       "      <td>-14.96</td>\n",
       "      <td>0.11</td>\n",
       "      <td>19.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>46.57</td>\n",
       "      <td>58.50</td>\n",
       "      <td>44.17</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.030</td>\n",
       "      <td>2.56</td>\n",
       "      <td>-22.19</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>19.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>46.57</td>\n",
       "      <td>46.80</td>\n",
       "      <td>39.26</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2.270</td>\n",
       "      <td>2.26</td>\n",
       "      <td>-8.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>14.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>46.57</td>\n",
       "      <td>35.10</td>\n",
       "      <td>58.89</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.515</td>\n",
       "      <td>2.32</td>\n",
       "      <td>-10.89</td>\n",
       "      <td>0.13</td>\n",
       "      <td>17.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>46.57</td>\n",
       "      <td>23.40</td>\n",
       "      <td>53.83</td>\n",
       "      <td>0.37</td>\n",
       "      <td>4.545</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-29.79</td>\n",
       "      <td>0.15</td>\n",
       "      <td>16.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>44.71</td>\n",
       "      <td>62.71</td>\n",
       "      <td>53.89</td>\n",
       "      <td>0.25</td>\n",
       "      <td>8.970</td>\n",
       "      <td>2.43</td>\n",
       "      <td>-22.46</td>\n",
       "      <td>0.02</td>\n",
       "      <td>33.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>44.71</td>\n",
       "      <td>57.12</td>\n",
       "      <td>49.39</td>\n",
       "      <td>0.37</td>\n",
       "      <td>8.220</td>\n",
       "      <td>2.43</td>\n",
       "      <td>-19.72</td>\n",
       "      <td>0.02</td>\n",
       "      <td>33.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>44.71</td>\n",
       "      <td>51.53</td>\n",
       "      <td>44.89</td>\n",
       "      <td>0.50</td>\n",
       "      <td>7.470</td>\n",
       "      <td>2.43</td>\n",
       "      <td>-14.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>33.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>44.71</td>\n",
       "      <td>45.94</td>\n",
       "      <td>40.39</td>\n",
       "      <td>0.62</td>\n",
       "      <td>6.720</td>\n",
       "      <td>3.98</td>\n",
       "      <td>-28.34</td>\n",
       "      <td>-1.53</td>\n",
       "      <td>-3.694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44.71</td>\n",
       "      <td>45.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.980</td>\n",
       "      <td>5.06</td>\n",
       "      <td>-24.23</td>\n",
       "      <td>-2.61</td>\n",
       "      <td>-8.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>39.12</td>\n",
       "      <td>62.71</td>\n",
       "      <td>49.39</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6.720</td>\n",
       "      <td>2.45</td>\n",
       "      <td>-10.59</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>45.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>39.12</td>\n",
       "      <td>57.12</td>\n",
       "      <td>44.89</td>\n",
       "      <td>0.62</td>\n",
       "      <td>5.980</td>\n",
       "      <td>2.45</td>\n",
       "      <td>-7.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>45.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>39.12</td>\n",
       "      <td>51.53</td>\n",
       "      <td>40.39</td>\n",
       "      <td>0.75</td>\n",
       "      <td>8.970</td>\n",
       "      <td>2.46</td>\n",
       "      <td>-7.41</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>40.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>39.12</td>\n",
       "      <td>45.94</td>\n",
       "      <td>36.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>8.220</td>\n",
       "      <td>2.54</td>\n",
       "      <td>-3.49</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>20.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>39.12</td>\n",
       "      <td>40.35</td>\n",
       "      <td>53.89</td>\n",
       "      <td>0.37</td>\n",
       "      <td>7.470</td>\n",
       "      <td>2.44</td>\n",
       "      <td>-32.66</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>37.26</td>\n",
       "      <td>70.20</td>\n",
       "      <td>44.17</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.515</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-7.64</td>\n",
       "      <td>0.15</td>\n",
       "      <td>16.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>37.26</td>\n",
       "      <td>58.50</td>\n",
       "      <td>39.26</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4.545</td>\n",
       "      <td>2.36</td>\n",
       "      <td>-23.70</td>\n",
       "      <td>0.09</td>\n",
       "      <td>20.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>37.26</td>\n",
       "      <td>46.80</td>\n",
       "      <td>58.89</td>\n",
       "      <td>0.37</td>\n",
       "      <td>3.790</td>\n",
       "      <td>2.36</td>\n",
       "      <td>-16.44</td>\n",
       "      <td>0.09</td>\n",
       "      <td>20.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>37.26</td>\n",
       "      <td>35.10</td>\n",
       "      <td>53.83</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.030</td>\n",
       "      <td>2.38</td>\n",
       "      <td>-14.39</td>\n",
       "      <td>0.07</td>\n",
       "      <td>23.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>37.26</td>\n",
       "      <td>23.40</td>\n",
       "      <td>49.07</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.270</td>\n",
       "      <td>2.40</td>\n",
       "      <td>-10.76</td>\n",
       "      <td>0.05</td>\n",
       "      <td>26.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>33.53</td>\n",
       "      <td>62.71</td>\n",
       "      <td>44.89</td>\n",
       "      <td>0.75</td>\n",
       "      <td>8.220</td>\n",
       "      <td>2.48</td>\n",
       "      <td>-7.53</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>30.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>33.53</td>\n",
       "      <td>57.12</td>\n",
       "      <td>40.39</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7.470</td>\n",
       "      <td>2.48</td>\n",
       "      <td>-4.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>30.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>33.53</td>\n",
       "      <td>51.53</td>\n",
       "      <td>36.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>6.720</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-2.80</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>18.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>33.53</td>\n",
       "      <td>45.94</td>\n",
       "      <td>53.89</td>\n",
       "      <td>0.50</td>\n",
       "      <td>5.980</td>\n",
       "      <td>2.46</td>\n",
       "      <td>-11.77</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>40.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>33.53</td>\n",
       "      <td>40.35</td>\n",
       "      <td>49.39</td>\n",
       "      <td>0.62</td>\n",
       "      <td>8.970</td>\n",
       "      <td>2.46</td>\n",
       "      <td>-17.39</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>40.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>27.94</td>\n",
       "      <td>70.20</td>\n",
       "      <td>39.26</td>\n",
       "      <td>0.37</td>\n",
       "      <td>3.030</td>\n",
       "      <td>2.42</td>\n",
       "      <td>-23.81</td>\n",
       "      <td>0.03</td>\n",
       "      <td>30.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>27.94</td>\n",
       "      <td>62.71</td>\n",
       "      <td>40.39</td>\n",
       "      <td>0.37</td>\n",
       "      <td>5.980</td>\n",
       "      <td>4.91</td>\n",
       "      <td>-34.12</td>\n",
       "      <td>-2.46</td>\n",
       "      <td>-7.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>27.94</td>\n",
       "      <td>58.50</td>\n",
       "      <td>58.89</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.270</td>\n",
       "      <td>2.40</td>\n",
       "      <td>-29.32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>26.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>27.94</td>\n",
       "      <td>57.12</td>\n",
       "      <td>36.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>8.970</td>\n",
       "      <td>4.83</td>\n",
       "      <td>-20.66</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>-7.532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>27.94</td>\n",
       "      <td>51.53</td>\n",
       "      <td>53.89</td>\n",
       "      <td>0.62</td>\n",
       "      <td>8.220</td>\n",
       "      <td>2.50</td>\n",
       "      <td>-9.39</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>26.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>27.94</td>\n",
       "      <td>46.80</td>\n",
       "      <td>53.83</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.515</td>\n",
       "      <td>2.38</td>\n",
       "      <td>-8.91</td>\n",
       "      <td>0.07</td>\n",
       "      <td>23.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>27.94</td>\n",
       "      <td>45.94</td>\n",
       "      <td>49.39</td>\n",
       "      <td>0.75</td>\n",
       "      <td>7.470</td>\n",
       "      <td>2.50</td>\n",
       "      <td>-7.50</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>26.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>27.94</td>\n",
       "      <td>40.35</td>\n",
       "      <td>44.89</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6.720</td>\n",
       "      <td>4.86</td>\n",
       "      <td>-15.08</td>\n",
       "      <td>-2.41</td>\n",
       "      <td>-7.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>27.94</td>\n",
       "      <td>35.10</td>\n",
       "      <td>49.07</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.545</td>\n",
       "      <td>2.44</td>\n",
       "      <td>-28.71</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>27.94</td>\n",
       "      <td>23.40</td>\n",
       "      <td>44.17</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3.790</td>\n",
       "      <td>2.42</td>\n",
       "      <td>-23.69</td>\n",
       "      <td>0.03</td>\n",
       "      <td>30.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>22.35</td>\n",
       "      <td>62.71</td>\n",
       "      <td>36.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>7.470</td>\n",
       "      <td>2.63</td>\n",
       "      <td>-1.86</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>14.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>22.35</td>\n",
       "      <td>57.12</td>\n",
       "      <td>53.89</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.720</td>\n",
       "      <td>2.53</td>\n",
       "      <td>-5.15</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>21.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>22.35</td>\n",
       "      <td>51.53</td>\n",
       "      <td>49.39</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5.980</td>\n",
       "      <td>2.54</td>\n",
       "      <td>-3.55</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>20.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>22.35</td>\n",
       "      <td>45.94</td>\n",
       "      <td>44.89</td>\n",
       "      <td>0.37</td>\n",
       "      <td>8.970</td>\n",
       "      <td>2.54</td>\n",
       "      <td>-3.81</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>20.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>22.35</td>\n",
       "      <td>40.35</td>\n",
       "      <td>40.39</td>\n",
       "      <td>0.50</td>\n",
       "      <td>8.220</td>\n",
       "      <td>2.55</td>\n",
       "      <td>-2.23</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>20.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>18.63</td>\n",
       "      <td>70.20</td>\n",
       "      <td>58.89</td>\n",
       "      <td>0.75</td>\n",
       "      <td>4.545</td>\n",
       "      <td>2.58</td>\n",
       "      <td>-12.70</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>17.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>18.63</td>\n",
       "      <td>58.50</td>\n",
       "      <td>53.83</td>\n",
       "      <td>0.62</td>\n",
       "      <td>3.790</td>\n",
       "      <td>2.54</td>\n",
       "      <td>-10.96</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>20.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>18.63</td>\n",
       "      <td>46.80</td>\n",
       "      <td>49.07</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3.030</td>\n",
       "      <td>2.48</td>\n",
       "      <td>-11.80</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>30.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>18.63</td>\n",
       "      <td>35.10</td>\n",
       "      <td>44.17</td>\n",
       "      <td>0.37</td>\n",
       "      <td>2.270</td>\n",
       "      <td>2.48</td>\n",
       "      <td>-20.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>30.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>18.63</td>\n",
       "      <td>23.40</td>\n",
       "      <td>39.26</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.515</td>\n",
       "      <td>2.44</td>\n",
       "      <td>-24.93</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       P1     P2     P3    P4     P5  Resultant Frequency  Return Loss  \\\n",
       "0   55.89  70.20  53.83  0.50  2.270                 2.58       -18.99   \n",
       "1   55.89  58.50  49.07  0.37  1.515                 2.58       -10.89   \n",
       "2   55.89  46.80  44.17  0.25  4.545                 2.32       -19.98   \n",
       "3   55.89  35.10  39.26  0.75  3.790                 2.48       -18.94   \n",
       "4   55.89  23.40  58.89  0.62  3.030                 2.66       -22.45   \n",
       "5   46.57  70.20  49.07  0.25  3.790                 2.34       -14.96   \n",
       "6   46.57  58.50  44.17  0.75  3.030                 2.56       -22.19   \n",
       "7   46.57  46.80  39.26  0.62  2.270                 2.26        -8.08   \n",
       "8   46.57  35.10  58.89  0.50  1.515                 2.32       -10.89   \n",
       "9   46.57  23.40  53.83  0.37  4.545                 2.30       -29.79   \n",
       "10  44.71  62.71  53.89  0.25  8.970                 2.43       -22.46   \n",
       "11  44.71  57.12  49.39  0.37  8.220                 2.43       -19.72   \n",
       "12  44.71  51.53  44.89  0.50  7.470                 2.43       -14.01   \n",
       "13  44.71  45.94  40.39  0.62  6.720                 3.98       -28.34   \n",
       "14  44.71  45.00  36.00  0.75  5.980                 5.06       -24.23   \n",
       "15  39.12  62.71  49.39  0.50  6.720                 2.45       -10.59   \n",
       "16  39.12  57.12  44.89  0.62  5.980                 2.45        -7.00   \n",
       "17  39.12  51.53  40.39  0.75  8.970                 2.46        -7.41   \n",
       "18  39.12  45.94  36.00  0.25  8.220                 2.54        -3.49   \n",
       "19  39.12  40.35  53.89  0.37  7.470                 2.44       -32.66   \n",
       "20  37.26  70.20  44.17  0.62  1.515                 2.30        -7.64   \n",
       "21  37.26  58.50  39.26  0.50  4.545                 2.36       -23.70   \n",
       "22  37.26  46.80  58.89  0.37  3.790                 2.36       -16.44   \n",
       "23  37.26  35.10  53.83  0.25  3.030                 2.38       -14.39   \n",
       "24  37.26  23.40  49.07  0.75  2.270                 2.40       -10.76   \n",
       "25  33.53  62.71  44.89  0.75  8.220                 2.48        -7.53   \n",
       "26  33.53  57.12  40.39  0.25  7.470                 2.48        -4.03   \n",
       "27  33.53  51.53  36.00  0.37  6.720                 2.57        -2.80   \n",
       "28  33.53  45.94  53.89  0.50  5.980                 2.46       -11.77   \n",
       "29  33.53  40.35  49.39  0.62  8.970                 2.46       -17.39   \n",
       "30  27.94  70.20  39.26  0.37  3.030                 2.42       -23.81   \n",
       "31  27.94  62.71  40.39  0.37  5.980                 4.91       -34.12   \n",
       "32  27.94  58.50  58.89  0.25  2.270                 2.40       -29.32   \n",
       "33  27.94  57.12  36.00  0.50  8.970                 4.83       -20.66   \n",
       "34  27.94  51.53  53.89  0.62  8.220                 2.50        -9.39   \n",
       "35  27.94  46.80  53.83  0.75  1.515                 2.38        -8.91   \n",
       "36  27.94  45.94  49.39  0.75  7.470                 2.50        -7.50   \n",
       "37  27.94  40.35  44.89  0.25  6.720                 4.86       -15.08   \n",
       "38  27.94  35.10  49.07  0.62  4.545                 2.44       -28.71   \n",
       "39  27.94  23.40  44.17  0.50  3.790                 2.42       -23.69   \n",
       "40  22.35  62.71  36.00  0.62  7.470                 2.63        -1.86   \n",
       "41  22.35  57.12  53.89  0.75  6.720                 2.53        -5.15   \n",
       "42  22.35  51.53  49.39  0.25  5.980                 2.54        -3.55   \n",
       "43  22.35  45.94  44.89  0.37  8.970                 2.54        -3.81   \n",
       "44  22.35  40.35  40.39  0.50  8.220                 2.55        -2.23   \n",
       "45  18.63  70.20  58.89  0.75  4.545                 2.58       -12.70   \n",
       "46  18.63  58.50  53.83  0.62  3.790                 2.54       -10.96   \n",
       "47  18.63  46.80  49.07  0.50  3.030                 2.48       -11.80   \n",
       "48  18.63  35.10  44.17  0.37  2.270                 2.48       -20.02   \n",
       "49  18.63  23.40  39.26  0.25  1.515                 2.44       -24.93   \n",
       "\n",
       "    Fitness Function  S/N ratio  \n",
       "0              -0.13     17.720  \n",
       "1              -0.13     17.720  \n",
       "2               0.13     17.720  \n",
       "3              -0.03     30.460  \n",
       "4              -0.21     13.550  \n",
       "5               0.11     19.170  \n",
       "6              -0.11     19.170  \n",
       "7               0.19     14.420  \n",
       "8               0.13     17.720  \n",
       "9               0.15     16.480  \n",
       "10              0.02     33.980  \n",
       "11              0.02     33.980  \n",
       "12              0.02     33.980  \n",
       "13             -1.53     -3.694  \n",
       "14             -2.61     -8.333  \n",
       "15             -0.01     45.040  \n",
       "16             -0.01     45.040  \n",
       "17             -0.01     40.000  \n",
       "18             -0.09     20.920  \n",
       "19              0.01     40.000  \n",
       "20              0.15     16.480  \n",
       "21              0.09     20.910  \n",
       "22              0.09     20.910  \n",
       "23              0.07     23.100  \n",
       "24              0.05     26.020  \n",
       "25             -0.03     30.460  \n",
       "26             -0.03     30.460  \n",
       "27             -0.12     18.420  \n",
       "28             -0.01     40.000  \n",
       "29             -0.01     40.000  \n",
       "30              0.03     30.460  \n",
       "31             -2.46     -7.819  \n",
       "32              0.05     26.020  \n",
       "33             -2.38     -7.532  \n",
       "34             -0.05     26.020  \n",
       "35              0.07     23.100  \n",
       "36             -0.05     26.020  \n",
       "37             -2.41     -7.640  \n",
       "38              0.01     40.000  \n",
       "39              0.03     30.460  \n",
       "40             -0.18     14.890  \n",
       "41             -0.08     21.940  \n",
       "42             -0.09     20.920  \n",
       "43             -0.09     20.920  \n",
       "44             -0.10     20.000  \n",
       "45             -0.13     17.720  \n",
       "46             -0.09     20.910  \n",
       "47             -0.03     30.460  \n",
       "48             -0.03     30.460  \n",
       "49              0.01     40.000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_excel('OA3.xlsx',sheet_name=\"Sheet1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4ba6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              P1         P2         P3         P4         P5  \\\n",
      "count  50.000000  50.000000  50.000000  50.000000  50.000000   \n",
      "mean   35.394000  49.258000  46.978000   0.498000   5.251000   \n",
      "std    11.134238  13.267866   7.017078   0.178589   2.488117   \n",
      "min    18.630000  23.400000  36.000000   0.250000   1.515000   \n",
      "25%    27.940000  40.350000  40.390000   0.370000   3.030000   \n",
      "50%    35.395000  49.165000  46.980000   0.500000   5.262500   \n",
      "75%    44.710000  58.500000  53.830000   0.620000   7.470000   \n",
      "max    55.890000  70.200000  58.890000   0.750000   8.970000   \n",
      "\n",
      "       Resultant Frequency  Return Loss  Fitness Function  S/N ratio  \n",
      "count            50.000000    50.000000         50.000000  50.000000  \n",
      "mean              2.685800   -15.234400         -0.236200  22.982240  \n",
      "std               0.703789     8.798548          0.703655  13.207934  \n",
      "min               2.260000   -34.120000         -2.610000  -8.333000  \n",
      "25%               2.420000   -22.385000         -0.097500  17.720000  \n",
      "50%               2.470000   -14.200000         -0.020000  21.430000  \n",
      "75%               2.547500    -7.750000          0.030000  30.460000  \n",
      "max               5.060000    -1.860000          0.190000  45.040000  \n"
     ]
    }
   ],
   "source": [
    "ant = pd.read_excel('OA3.xlsx',sheet_name=\"Sheet1\")\n",
    "print(ant.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aae7a0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.589e+01,  7.020e+01,  5.383e+01,  5.000e-01,  2.270e+00,\n",
       "         2.580e+00, -1.899e+01, -1.300e-01,  1.772e+01],\n",
       "       [ 5.589e+01,  5.850e+01,  4.907e+01,  3.700e-01,  1.515e+00,\n",
       "         2.580e+00, -1.089e+01, -1.300e-01,  1.772e+01],\n",
       "       [ 5.589e+01,  4.680e+01,  4.417e+01,  2.500e-01,  4.545e+00,\n",
       "         2.320e+00, -1.998e+01,  1.300e-01,  1.772e+01],\n",
       "       [ 5.589e+01,  3.510e+01,  3.926e+01,  7.500e-01,  3.790e+00,\n",
       "         2.480e+00, -1.894e+01, -3.000e-02,  3.046e+01],\n",
       "       [ 5.589e+01,  2.340e+01,  5.889e+01,  6.200e-01,  3.030e+00,\n",
       "         2.660e+00, -2.245e+01, -2.100e-01,  1.355e+01],\n",
       "       [ 4.657e+01,  7.020e+01,  4.907e+01,  2.500e-01,  3.790e+00,\n",
       "         2.340e+00, -1.496e+01,  1.100e-01,  1.917e+01],\n",
       "       [ 4.657e+01,  5.850e+01,  4.417e+01,  7.500e-01,  3.030e+00,\n",
       "         2.560e+00, -2.219e+01, -1.100e-01,  1.917e+01],\n",
       "       [ 4.657e+01,  4.680e+01,  3.926e+01,  6.200e-01,  2.270e+00,\n",
       "         2.260e+00, -8.080e+00,  1.900e-01,  1.442e+01],\n",
       "       [ 4.657e+01,  3.510e+01,  5.889e+01,  5.000e-01,  1.515e+00,\n",
       "         2.320e+00, -1.089e+01,  1.300e-01,  1.772e+01],\n",
       "       [ 4.657e+01,  2.340e+01,  5.383e+01,  3.700e-01,  4.545e+00,\n",
       "         2.300e+00, -2.979e+01,  1.500e-01,  1.648e+01],\n",
       "       [ 4.471e+01,  6.271e+01,  5.389e+01,  2.500e-01,  8.970e+00,\n",
       "         2.430e+00, -2.246e+01,  2.000e-02,  3.398e+01],\n",
       "       [ 4.471e+01,  5.712e+01,  4.939e+01,  3.700e-01,  8.220e+00,\n",
       "         2.430e+00, -1.972e+01,  2.000e-02,  3.398e+01],\n",
       "       [ 4.471e+01,  5.153e+01,  4.489e+01,  5.000e-01,  7.470e+00,\n",
       "         2.430e+00, -1.401e+01,  2.000e-02,  3.398e+01],\n",
       "       [ 4.471e+01,  4.594e+01,  4.039e+01,  6.200e-01,  6.720e+00,\n",
       "         3.980e+00, -2.834e+01, -1.530e+00, -3.694e+00],\n",
       "       [ 4.471e+01,  4.500e+01,  3.600e+01,  7.500e-01,  5.980e+00,\n",
       "         5.060e+00, -2.423e+01, -2.610e+00, -8.333e+00],\n",
       "       [ 3.912e+01,  6.271e+01,  4.939e+01,  5.000e-01,  6.720e+00,\n",
       "         2.450e+00, -1.059e+01, -1.000e-02,  4.504e+01],\n",
       "       [ 3.912e+01,  5.712e+01,  4.489e+01,  6.200e-01,  5.980e+00,\n",
       "         2.450e+00, -7.000e+00, -1.000e-02,  4.504e+01],\n",
       "       [ 3.912e+01,  5.153e+01,  4.039e+01,  7.500e-01,  8.970e+00,\n",
       "         2.460e+00, -7.410e+00, -1.000e-02,  4.000e+01],\n",
       "       [ 3.912e+01,  4.594e+01,  3.600e+01,  2.500e-01,  8.220e+00,\n",
       "         2.540e+00, -3.490e+00, -9.000e-02,  2.092e+01],\n",
       "       [ 3.912e+01,  4.035e+01,  5.389e+01,  3.700e-01,  7.470e+00,\n",
       "         2.440e+00, -3.266e+01,  1.000e-02,  4.000e+01],\n",
       "       [ 3.726e+01,  7.020e+01,  4.417e+01,  6.200e-01,  1.515e+00,\n",
       "         2.300e+00, -7.640e+00,  1.500e-01,  1.648e+01],\n",
       "       [ 3.726e+01,  5.850e+01,  3.926e+01,  5.000e-01,  4.545e+00,\n",
       "         2.360e+00, -2.370e+01,  9.000e-02,  2.091e+01],\n",
       "       [ 3.726e+01,  4.680e+01,  5.889e+01,  3.700e-01,  3.790e+00,\n",
       "         2.360e+00, -1.644e+01,  9.000e-02,  2.091e+01],\n",
       "       [ 3.726e+01,  3.510e+01,  5.383e+01,  2.500e-01,  3.030e+00,\n",
       "         2.380e+00, -1.439e+01,  7.000e-02,  2.310e+01],\n",
       "       [ 3.726e+01,  2.340e+01,  4.907e+01,  7.500e-01,  2.270e+00,\n",
       "         2.400e+00, -1.076e+01,  5.000e-02,  2.602e+01],\n",
       "       [ 3.353e+01,  6.271e+01,  4.489e+01,  7.500e-01,  8.220e+00,\n",
       "         2.480e+00, -7.530e+00, -3.000e-02,  3.046e+01],\n",
       "       [ 3.353e+01,  5.712e+01,  4.039e+01,  2.500e-01,  7.470e+00,\n",
       "         2.480e+00, -4.030e+00, -3.000e-02,  3.046e+01],\n",
       "       [ 3.353e+01,  5.153e+01,  3.600e+01,  3.700e-01,  6.720e+00,\n",
       "         2.570e+00, -2.800e+00, -1.200e-01,  1.842e+01],\n",
       "       [ 3.353e+01,  4.594e+01,  5.389e+01,  5.000e-01,  5.980e+00,\n",
       "         2.460e+00, -1.177e+01, -1.000e-02,  4.000e+01],\n",
       "       [ 3.353e+01,  4.035e+01,  4.939e+01,  6.200e-01,  8.970e+00,\n",
       "         2.460e+00, -1.739e+01, -1.000e-02,  4.000e+01],\n",
       "       [ 2.794e+01,  7.020e+01,  3.926e+01,  3.700e-01,  3.030e+00,\n",
       "         2.420e+00, -2.381e+01,  3.000e-02,  3.046e+01],\n",
       "       [ 2.794e+01,  6.271e+01,  4.039e+01,  3.700e-01,  5.980e+00,\n",
       "         4.910e+00, -3.412e+01, -2.460e+00, -7.819e+00],\n",
       "       [ 2.794e+01,  5.850e+01,  5.889e+01,  2.500e-01,  2.270e+00,\n",
       "         2.400e+00, -2.932e+01,  5.000e-02,  2.602e+01],\n",
       "       [ 2.794e+01,  5.712e+01,  3.600e+01,  5.000e-01,  8.970e+00,\n",
       "         4.830e+00, -2.066e+01, -2.380e+00, -7.532e+00],\n",
       "       [ 2.794e+01,  5.153e+01,  5.389e+01,  6.200e-01,  8.220e+00,\n",
       "         2.500e+00, -9.390e+00, -5.000e-02,  2.602e+01],\n",
       "       [ 2.794e+01,  4.680e+01,  5.383e+01,  7.500e-01,  1.515e+00,\n",
       "         2.380e+00, -8.910e+00,  7.000e-02,  2.310e+01],\n",
       "       [ 2.794e+01,  4.594e+01,  4.939e+01,  7.500e-01,  7.470e+00,\n",
       "         2.500e+00, -7.500e+00, -5.000e-02,  2.602e+01],\n",
       "       [ 2.794e+01,  4.035e+01,  4.489e+01,  2.500e-01,  6.720e+00,\n",
       "         4.860e+00, -1.508e+01, -2.410e+00, -7.640e+00],\n",
       "       [ 2.794e+01,  3.510e+01,  4.907e+01,  6.200e-01,  4.545e+00,\n",
       "         2.440e+00, -2.871e+01,  1.000e-02,  4.000e+01],\n",
       "       [ 2.794e+01,  2.340e+01,  4.417e+01,  5.000e-01,  3.790e+00,\n",
       "         2.420e+00, -2.369e+01,  3.000e-02,  3.046e+01],\n",
       "       [ 2.235e+01,  6.271e+01,  3.600e+01,  6.200e-01,  7.470e+00,\n",
       "         2.630e+00, -1.860e+00, -1.800e-01,  1.489e+01],\n",
       "       [ 2.235e+01,  5.712e+01,  5.389e+01,  7.500e-01,  6.720e+00,\n",
       "         2.530e+00, -5.150e+00, -8.000e-02,  2.194e+01],\n",
       "       [ 2.235e+01,  5.153e+01,  4.939e+01,  2.500e-01,  5.980e+00,\n",
       "         2.540e+00, -3.550e+00, -9.000e-02,  2.092e+01],\n",
       "       [ 2.235e+01,  4.594e+01,  4.489e+01,  3.700e-01,  8.970e+00,\n",
       "         2.540e+00, -3.810e+00, -9.000e-02,  2.092e+01],\n",
       "       [ 2.235e+01,  4.035e+01,  4.039e+01,  5.000e-01,  8.220e+00,\n",
       "         2.550e+00, -2.230e+00, -1.000e-01,  2.000e+01],\n",
       "       [ 1.863e+01,  7.020e+01,  5.889e+01,  7.500e-01,  4.545e+00,\n",
       "         2.580e+00, -1.270e+01, -1.300e-01,  1.772e+01],\n",
       "       [ 1.863e+01,  5.850e+01,  5.383e+01,  6.200e-01,  3.790e+00,\n",
       "         2.540e+00, -1.096e+01, -9.000e-02,  2.091e+01],\n",
       "       [ 1.863e+01,  4.680e+01,  4.907e+01,  5.000e-01,  3.030e+00,\n",
       "         2.480e+00, -1.180e+01, -3.000e-02,  3.046e+01],\n",
       "       [ 1.863e+01,  3.510e+01,  4.417e+01,  3.700e-01,  2.270e+00,\n",
       "         2.480e+00, -2.002e+01, -3.000e-02,  3.046e+01],\n",
       "       [ 1.863e+01,  2.340e+01,  3.926e+01,  2.500e-01,  1.515e+00,\n",
       "         2.440e+00, -2.493e+01,  1.000e-02,  4.000e+01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ant.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc2e74f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    -18.99\n",
       "1    -10.89\n",
       "2    -19.98\n",
       "3    -18.94\n",
       "4    -22.45\n",
       "5    -14.96\n",
       "6    -22.19\n",
       "7     -8.08\n",
       "8    -10.89\n",
       "9    -29.79\n",
       "10   -22.46\n",
       "11   -19.72\n",
       "12   -14.01\n",
       "13   -28.34\n",
       "14   -24.23\n",
       "15   -10.59\n",
       "16    -7.00\n",
       "17    -7.41\n",
       "18    -3.49\n",
       "19   -32.66\n",
       "20    -7.64\n",
       "21   -23.70\n",
       "22   -16.44\n",
       "23   -14.39\n",
       "24   -10.76\n",
       "25    -7.53\n",
       "26    -4.03\n",
       "27    -2.80\n",
       "28   -11.77\n",
       "29   -17.39\n",
       "30   -23.81\n",
       "31   -34.12\n",
       "32   -29.32\n",
       "33   -20.66\n",
       "34    -9.39\n",
       "35    -8.91\n",
       "36    -7.50\n",
       "37   -15.08\n",
       "38   -28.71\n",
       "39   -23.69\n",
       "40    -1.86\n",
       "41    -5.15\n",
       "42    -3.55\n",
       "43    -3.81\n",
       "44    -2.23\n",
       "45   -12.70\n",
       "46   -10.96\n",
       "47   -11.80\n",
       "48   -20.02\n",
       "49   -24.93\n",
       "Name: Return Loss, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=ant.drop(columns=['Resultant Frequency','Return Loss','Fitness Function','S/N ratio'])\n",
    "y=ant['Resultant Frequency']\n",
    "z=ant['Return Loss']\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c269db5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.37389919, 4.46961989])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting resultant frequency \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.2)\n",
    "model=LinearRegression()\n",
    "model.fit(x,y)\n",
    "p = model.predict([[1,2,5,4,8],[3,1,2,5,7]])\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b229748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-14.91727525,  26.97403212])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting return loss\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,z,test_size=0.2)\n",
    "model=LinearRegression()\n",
    "model.fit(x,z)\n",
    "p = model.predict([[55,69,54,0.75,4],[3,1,2,5,9]])\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "963b4865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0430457800374693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.55470712, 2.63294748, 2.56486432, 2.61213233, 2.60178686])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting resultant frequency\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.2)\n",
    "model=LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "p1 = model.predict(X_test)\n",
    "p = model.predict([[17,20,38,0.2,1.5],\n",
    "                   [18,19,35,0.3,1.6],\n",
    "                  [16,18,37,0.2,1.4],\n",
    "                  [17.5,19.5,36,0.3,1.45],\n",
    "                  [16.5,20,36.5,0.25,1.55]])\n",
    "# print(X_test,p)\n",
    "mse = mean_squared_error(y_test, p1)\n",
    "print(mse)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eaf0ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144.99580483181637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-19.79077367, -18.66246967, -19.84726011, -18.78396437,\n",
       "       -18.9262859 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting return loss\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X_train,X_test,z_train,z_test = train_test_split(x,z,test_size=0.2)\n",
    "model=LinearRegression()\n",
    "model.fit(X_train,z_train)\n",
    "p = model.predict(X_test)\n",
    "p1 = model.predict([[17,20,38,0.2,1.5],\n",
    "                   [18,19,35,0.3,1.6],\n",
    "                  [16,18,37,0.2,1.4],\n",
    "                  [17.5,19.5,36,0.3,1.45],\n",
    "                  [16.5,20,36.5,0.25,1.55]])\n",
    "# print(X_test,p)\n",
    "mse = mean_squared_error(z_test, p)\n",
    "print(mse)\n",
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04ef102a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resultant Frequency</th>\n",
       "      <th>Return Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.58</td>\n",
       "      <td>-18.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.58</td>\n",
       "      <td>-10.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.32</td>\n",
       "      <td>-19.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.48</td>\n",
       "      <td>-18.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.66</td>\n",
       "      <td>-22.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.34</td>\n",
       "      <td>-14.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.56</td>\n",
       "      <td>-22.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.26</td>\n",
       "      <td>-8.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.32</td>\n",
       "      <td>-10.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.30</td>\n",
       "      <td>-29.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.43</td>\n",
       "      <td>-22.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.43</td>\n",
       "      <td>-19.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.43</td>\n",
       "      <td>-14.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.98</td>\n",
       "      <td>-28.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.06</td>\n",
       "      <td>-24.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.45</td>\n",
       "      <td>-10.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.45</td>\n",
       "      <td>-7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.46</td>\n",
       "      <td>-7.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.54</td>\n",
       "      <td>-3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.44</td>\n",
       "      <td>-32.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.30</td>\n",
       "      <td>-7.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.36</td>\n",
       "      <td>-23.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.36</td>\n",
       "      <td>-16.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.38</td>\n",
       "      <td>-14.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.40</td>\n",
       "      <td>-10.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.48</td>\n",
       "      <td>-7.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.48</td>\n",
       "      <td>-4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.57</td>\n",
       "      <td>-2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.46</td>\n",
       "      <td>-11.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.46</td>\n",
       "      <td>-17.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.42</td>\n",
       "      <td>-23.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.91</td>\n",
       "      <td>-34.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.40</td>\n",
       "      <td>-29.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4.83</td>\n",
       "      <td>-20.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2.50</td>\n",
       "      <td>-9.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2.38</td>\n",
       "      <td>-8.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2.50</td>\n",
       "      <td>-7.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.86</td>\n",
       "      <td>-15.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2.44</td>\n",
       "      <td>-28.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2.42</td>\n",
       "      <td>-23.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2.63</td>\n",
       "      <td>-1.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2.53</td>\n",
       "      <td>-5.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2.54</td>\n",
       "      <td>-3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.54</td>\n",
       "      <td>-3.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2.55</td>\n",
       "      <td>-2.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2.58</td>\n",
       "      <td>-12.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2.54</td>\n",
       "      <td>-10.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2.48</td>\n",
       "      <td>-11.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2.48</td>\n",
       "      <td>-20.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2.44</td>\n",
       "      <td>-24.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Resultant Frequency  Return Loss\n",
       "0                  2.58       -18.99\n",
       "1                  2.58       -10.89\n",
       "2                  2.32       -19.98\n",
       "3                  2.48       -18.94\n",
       "4                  2.66       -22.45\n",
       "5                  2.34       -14.96\n",
       "6                  2.56       -22.19\n",
       "7                  2.26        -8.08\n",
       "8                  2.32       -10.89\n",
       "9                  2.30       -29.79\n",
       "10                 2.43       -22.46\n",
       "11                 2.43       -19.72\n",
       "12                 2.43       -14.01\n",
       "13                 3.98       -28.34\n",
       "14                 5.06       -24.23\n",
       "15                 2.45       -10.59\n",
       "16                 2.45        -7.00\n",
       "17                 2.46        -7.41\n",
       "18                 2.54        -3.49\n",
       "19                 2.44       -32.66\n",
       "20                 2.30        -7.64\n",
       "21                 2.36       -23.70\n",
       "22                 2.36       -16.44\n",
       "23                 2.38       -14.39\n",
       "24                 2.40       -10.76\n",
       "25                 2.48        -7.53\n",
       "26                 2.48        -4.03\n",
       "27                 2.57        -2.80\n",
       "28                 2.46       -11.77\n",
       "29                 2.46       -17.39\n",
       "30                 2.42       -23.81\n",
       "31                 4.91       -34.12\n",
       "32                 2.40       -29.32\n",
       "33                 4.83       -20.66\n",
       "34                 2.50        -9.39\n",
       "35                 2.38        -8.91\n",
       "36                 2.50        -7.50\n",
       "37                 4.86       -15.08\n",
       "38                 2.44       -28.71\n",
       "39                 2.42       -23.69\n",
       "40                 2.63        -1.86\n",
       "41                 2.53        -5.15\n",
       "42                 2.54        -3.55\n",
       "43                 2.54        -3.81\n",
       "44                 2.55        -2.23\n",
       "45                 2.58       -12.70\n",
       "46                 2.54       -10.96\n",
       "47                 2.48       -11.80\n",
       "48                 2.48       -20.02\n",
       "49                 2.44       -24.93"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=ant.drop(columns=['Resultant Frequency','Return Loss','Fitness Function','S/N ratio'])\n",
    "b=ant.drop(columns=['P1','P2','P3','P4','P5','Fitness Function','S/N ratio'])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99817149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       P1     P2     P3    P4     P5\n",
      "32  27.94  58.50  58.89  0.25  2.270\n",
      "6   46.57  58.50  44.17  0.75  3.030\n",
      "28  33.53  45.94  53.89  0.50  5.980\n",
      "36  27.94  45.94  49.39  0.75  7.470\n",
      "44  22.35  40.35  40.39  0.50  8.220\n",
      "25  33.53  62.71  44.89  0.75  8.220\n",
      "49  18.63  23.40  39.26  0.25  1.515\n",
      "37  27.94  40.35  44.89  0.25  6.720\n",
      "35  27.94  46.80  53.83  0.75  1.515\n",
      "1   55.89  58.50  49.07  0.37  1.515 [[  1.96563981 -15.30653306]\n",
      " [  2.83079116 -15.46255667]\n",
      " [  2.46414369 -15.74120658]\n",
      " [  2.861627   -13.11283626]\n",
      " [  3.02564936 -13.30932746]\n",
      " [  3.07366602 -11.66232631]\n",
      " [  2.54010813 -17.09534069]\n",
      " [  2.62767556 -15.93964774]\n",
      " [  2.42117721 -14.39942605]\n",
      " [  2.32842272 -19.35627047]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29.934997239170443"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting resultant frequency and return loss\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from sklearn import tree\n",
    "a_train,a_test,b_train,b_test = train_test_split(a,b,test_size=0.2)\n",
    "model=LinearRegression()\n",
    "model.fit(a_train,b_train)\n",
    "p = model.predict(a_test)\n",
    "print(a_test,p)\n",
    "# tree.export_graphviz(model, out_file='ant-rec.dot',feature_names=['P1','P2','P3','P4','P5'],class_names = sorted(y.unique()),label='all',rounded=True,filled=True)\n",
    "mse = mean_squared_error(b_test, p)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e1467bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.099183503541546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  2.53450653, -18.42156267],\n",
       "       [  2.70753672, -17.64412376],\n",
       "       [  2.55988773, -18.37916051],\n",
       "       [  2.66834926, -17.61045465],\n",
       "       [  2.61681957, -17.72457843]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting resultant frequency and return loss with auto generated input parameters and 20% of train data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "a_train,a_test,b_train,b_test = train_test_split(a,b,test_size=0.2)\n",
    "model=LinearRegression()\n",
    "model.fit(a_train,b_train)\n",
    "p1 = model.predict(a_test)\n",
    "mse = mean_squared_error(b_test, p1)\n",
    "print(mse)\n",
    "p = model.predict([[17,20,38,0.2,1.5],\n",
    "                   [18,19,35,0.3,1.6],\n",
    "                  [16,18,37,0.2,1.4],\n",
    "                  [17.5,19.5,36,0.3,1.45],\n",
    "                  [16.5,20,36.5,0.25,1.55]])\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9438b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[40.90966605, 43.42360274, 50.7665342 ,  0.43501478,  3.60343715]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting optimal input parameters from output parameters given\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "b_train,b_test,a_train,a_test = train_test_split(b,a,test_size=0.2)\n",
    "model=LinearRegression()\n",
    "model.fit(b,a)\n",
    "p = model.predict([[2.45,-30]])\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71258673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Resultant Frequency  Return Loss\n",
      "11                 2.43       -19.72\n",
      "10                 2.43       -22.46\n",
      "25                 2.48        -7.53\n",
      "1                  2.58       -10.89\n",
      "18                 2.54        -3.49\n",
      "31                 4.91       -34.12\n",
      "41                 2.53        -5.15\n",
      "27                 2.57        -2.80\n",
      "45                 2.58       -12.70\n",
      "32                 2.40       -29.32 [[37.87319725 44.0590183  47.9813922   0.49590158  4.30446369]\n",
      " [39.09625467 42.54517607 48.21605687  0.48969298  4.05702096]\n",
      " [32.352331   50.88041422 46.74732023  0.52449199  5.4735917 ]\n",
      " [33.69293892 49.19693469 46.65494194  0.51881642  5.30671583]\n",
      " [30.45347104 53.21624934 46.17323295  0.53480901  5.9203688 ]\n",
      " [40.35279135 40.39125044 39.78713837  0.51133227  6.3906621 ]\n",
      " [31.2103674  52.28181295 46.35341628  0.53085381  5.75680248]\n",
      " [30.09771492 53.64934566 46.00009589  0.53695386  6.02364829]\n",
      " [34.50087101 48.19691482 46.80995766  0.51471512  5.14325914]\n",
      " [42.20612183 38.70317223 48.91761767  0.47356746  3.39654379]]\n"
     ]
    }
   ],
   "source": [
    "# predicting optimal input parameters from output parameters given (auto generated)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "b_train,b_test,a_train,a_test = train_test_split(b,a,test_size=0.2)\n",
    "model=LinearRegression()\n",
    "model.fit(b_train,a_train)\n",
    "p = model.predict(b_test)\n",
    "print(b_test,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7bc642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freq:2.45 gain:-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69e30e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
